import streamlit as st
from src.ui.utils import home_button, switch_page

from src.memory.chat_history import (
    init_history,
    clear_history,
    add_user_message,
    add_assistant_message,
    get_history,
)
from src.rag.retriever import retrieve_chunks
from src.rag.qa_chain import build_prompt
from src.llm.chat_model import stream_completion

def render_chat():
    # Barre du haut avec navigation et actions
    col1, col2, col3 = st.columns([1, 1, 4])

    with col1:
        home_button()  # Bouton retour Ã  lâ€™accueil

    with col2:
        st.button("âš™ï¸ Model settings", on_click=lambda: switch_page("Model"))  # AccÃ¨s aux rÃ©glages du modÃ¨le

    with col3:
        if st.button("ðŸ”„ Reset conversation"):  # RÃ©initialise la conversation
            clear_history(st.session_state)
            st.rerun()


    st.title("Chat with Othello")  # Titre de la page

    init_history(st.session_state)              # Initialisation de lâ€™historique si absent
    st.session_state.setdefault("is_streaming", False)  # Flag pour gÃ©rer lâ€™affichage pendant le streaming

    # Affichage de lâ€™historique du chat
    history = get_history(st.session_state)
    for i, msg in enumerate(history):
        # Ã‰vite de dupliquer le dernier message pendant le streaming
        if st.session_state["is_streaming"] and i == len(history) - 1:
            continue
        role, content = msg.split(":", 1)
        with st.chat_message(role.lower()):
            st.markdown(content)

    # Champ de saisie utilisateur
    user_input = st.chat_input("Ask a question about Othello")
    if not user_input:
        return

    # Affichage du message utilisateur
    add_user_message(st.session_state, user_input)
    with st.chat_message("user"):
        st.markdown(user_input)

    # RÃ©cupÃ©ration du contexte via le RAG
    chunks = retrieve_chunks(user_input, k=5)
    prompt = build_prompt(user_input, chunks, history)

    st.session_state["is_streaming"] = True  # DÃ©but du streaming

    # RÃ©ponse de lâ€™assistant en streaming
    with st.chat_message("assistant"): # CrÃ©e une bulle de message cÃ´tÃ© assistant
        response_box = st.empty()  # Zone Streamlit vide que lâ€™on mettra Ã  jour en continu
        buffer = ""     # Buffer qui accumule tous les tokens reÃ§us

        # Appel du modÃ¨le OpenAI en streaming token par token
        for token in stream_completion(
            messages=[{"role": "user", "content": prompt}],  # Prompt RAG complet
            model=st.session_state["model_name"],   # ModÃ¨le choisi dans les settings
            temperature=st.session_state["temperature"],   # TempÃ©rature choisie
        ):
            buffer += token   # On ajoute le token au texte dÃ©jÃ  reÃ§u

            # Pendant le streaming, on nâ€™affiche QUE la rÃ©ponse, on cache les sources
            visible = buffer.split("---SOURCES---")[0]
            response_box.markdown(visible)  # Mise Ã  jour de lâ€™affichage en temps rÃ©el

        # Une fois le streaming terminÃ©, on sÃ©pare la rÃ©ponse et les sources
        answer, *sources_part = buffer.split("---SOURCES---")
        clean = answer.strip() # Nettoyage de la rÃ©ponse finale

        # Si des sources existent, on les formate proprement abec des saut de ligne titre etc 
        if sources_part:
            sources = [
                s.strip() for s in sources_part[0].split("\n") if s.strip()
            ]
            if sources:
                clean += "\n\n**Sources:**\n"
                for s in sources:
                    clean += f"> {s}\n\n"

        # Remplacement du streaming brut par la version finale propre
        response_box.markdown(clean)

    # Sauvegarde de la rÃ©ponse propre dans lâ€™historique
    add_assistant_message(st.session_state, clean)
    st.session_state["is_streaming"] = False  # Fin du streaming
